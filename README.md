# Pic Scribe

## About the project

I made a image captioning model by using ViT as encoder and Transformer Decoder as decoder, and training it with COCO2017 dataset. 
Then deployed these models to iOS app by converting these models into CoreML and building an iOS app with SwiftUI.

Here are some useful links of this repo:
[Creating Image Captioning model using PyTorch](https://github.com/seungjun-green/PicScribe/blob/master/Make%20Image%20Captioner%20Model.ipynb)

[Converting two PyTorch models into CoreML models](https://github.com/seungjun-green/PicScribe/blob/master/Convert_PyTorch_Models_to_CoreML_Models.ipynb)

[CoreML model - encoder](https://github.com/seungjun-green/PicScribe/tree/master/Pic%20Scribe/Pic%20Scribe/VIT_iOS_Encoder_v10.mlpackage)

[CoreML model - decoder](https://github.com/seungjun-green/PicScribe/tree/master/Pic%20Scribe/Pic%20Scribe/iOS_Decoder_V14.mlpackage)

Plus you can download and test the model on HuggingFace too. HuggingFace


## More Details about the model

## How to use this project

Create a folder in ur computer and move to it then type this:
git clone

then go to cd PicScribe and open the xcode project and build it on Xcdoe.


if you're curious on how these models were created u can check out the notebooks.
